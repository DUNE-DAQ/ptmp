#+title: PTMP

Prototype Trigger Message Passing aka Pretty Temporary

* Introduction 

This package provides trigger messages transport for the protoDUNE DAQ
prototyping development toward full DUNE Far Detector DAQ.

A simple API is provided which hides details of the transport
mechanism.  The application simply sees a sink or a source of trigger
messages.  Driven by user configuration, an intervening network may be
defined.  This can be a simple sink-to-source pipeline or may be
structured into a rich data flow network which may include redundant
flow paths and intervening filtering/processing with redundancy and
load balancing.  The sink/source applications need not be complicated
by this richness.

Message transport is based on ZeroMQ which provides robustness and
performance (see the section on throughput performance in the [[./docs/tuning.org][tuning]]
document).  Thus, the PTMP network may exist over a mix of transports
(inter-thread, inter-process and inter-computer) as determined by the
configuration.  The payload message frames are currently serialized
using protobuf ([[./ptmp/ptmp.proto][schema]]).


* Status

This package has basic functionality.  See [[./docs/todo.org][To Do]] document for checklist
and notes on future development. 

This button determines if recent builds and tests pass on Travis-CI:

[[https://travis-ci.org/brettviren/ptmp][file:https://travis-ci.org/brettviren/ptmp.svg]]

Note, a failure here is not necessarily indicative of a real problem.

* Installation 

The PTMP package should build with any reasonably up to date C++
compiler.  GCC 7 is used in development.  Prerequisite software
dependencies are:

- libzmq
- CZMQ
- protobuf (v3)

See the [[./.travis.yml][Travis-CI file]] for an ~apt~ installation line.  There is also a
[[./installdeps.sh][script]] to assist building the dependencies from source.  With
prerequisites available, issue:

#+BEGIN_EXAMPLE
  $ ./waf --prefix=<path> configure build install
  $ ls <path>/{lib,include,bin}/
#+END_EXAMPLE 

Replace ~<path>~ with a into which you want the software to install.  If
you want to just test it locally you may omit ~--prefix~ and the ~install~
command.

The default build will run the tests.  To avoid that, for example,
while developing, you may do:

#+BEGIN_EXAMPLE
  $ ./waf --notests [install]
#+END_EXAMPLE 


See [[./docs/ups.org]] for special support for building in the UPS
ecosystem.

As part of the build, a number of tests will be built and run.  See
the [[./docs/tuning.org][tuning]] document for some details on these tests.

* Application Development

PTMP provides a very specific and almost trivial API.  Application may
be a "sender" or a "receiver" of trigger messages by instantiating a
~TPSender~ or ~TPReceiver~, respectively, with their configuration string.
These instances should be long-lived in the application and as they
hold a ZeroMQ socket should be used from the thread in which they were
constructed.  They provide /callable/ objects.  The sender will take a
~const TPSet&~ to read and send to the network as a single message and
the receiver takes a non-const ~TPSet&~ to be filled when a message
arrives.  The receiver also takes an optional second argument which is
a timeout in msec.  If no message arrives w/in that time the call will
return the value ~false~.  The receiver may ~throw std::runtime_error~ if
a message is received which can not be deserialized into the ~TPSet~
object.

The following applications provide reference applications.  Running
them without command line arguments will give a usage message.

- [[./test/check_send.cc][check_send]] :: send empty ~TPSet~ via ~ipc~ or ~tcp~ in a simple manner.
- [[./test/check_send.cc][check_send_rates]] :: a more sophisticated version of the above which
     adds a model for inter-message timing variability as well as
     filling ~TPSet~ with some number of actual TPs.
- [[./test/check_recv.cc][check_recv]] :: receive messages via ~ipc~ or ~tcp~.  Works with either senders.
- [[./test/check_sendrecv.cc][check_sendrecv]] :: a self contained sender+receiver that can use
     ~inproc~ as well as ~ipc~ or ~tcp~.

Some of the unit tests run by CI are:

- [[./test/test_sendrecv.sh]] :: run through all combinations of the supported transport and socket patterns for a 1-to-1 test.
- [[./test/test_many_senders.sh]] :: run through all supported transports with a many-to-1 test of PUB/SUB.  Note, this pattern is what a "TC finder" will likely follow.

Application programmers (and possibly their users) should also read
the section [[Configuration]] below.


* Configuration

The PTMP API classes are configured through a string in JSON format.
The JSON object must contain an attribute named ~socket~ which has a
value that is an object.  The ~socket~ object then may have the
following attributes:

- type :: a ZeroMQ socket type name ("PAIR", "PUB", "SUB", etc)
- bind :: an array of addresses in canonical ZeroMQ form that the socket should bind
- connect :: an array of addresses in canonical ZeroMQ form that the socket should connect

An example configuration string for a sender might look like:

#+BEGIN_SRC json
{ 
  "type": "PUB",
  "bind": [ "tcp://127.0.0.1:12345" ]
}
#+END_SRC

An example configuration string for a receiver connecting to two
senders might look like:

#+BEGIN_SRC json
{
  "type": "SUB",
  "connect": [ "tcp://127.0.0.1:12345", "tcp://127.0.0.1:12346" ]
}
#+END_SRC

What follows is some discussion on how selecting a configuration for
the PTMP API classes.  In short, the recommendation is:

- ~TPSender~ :: use type PUB and bind
- ~TPRecevier~ :: use type SUB and connect

** Attachment mode

In principle both ~bind~ and ~connect~ may be given to a single instance
(ZeroMQ supports this) but in practice it's likely best to designate
the *upstream* endpoint to ~bind~.  This makes upstream "server-like" and
downstream "client-like" (although the actual message transmission is
a linear flow, not request/reply).

** Socket type

Probably the two most important considerations in choosing the socket
type are

1) multiplicity and routing pattern
2) behavior when high water mark (HWM) is reached.

*** Multiplicity and Routing Patterns

PAIR sockets form only a 1-to-1 attachment (ZeroMQ 3 needs one
endpoint to ~bind~ before the other ~connect~. ZeroMQ 4 seems to have
removed this restriction).  As there is no multiplicity at either
endpoint there is no routing pattern to consider.  The pair of PAIR
sockets form a bidirectional pipe.

PUSH follows a round-robin distribution of messages to its PULL
endpoints.  Each subsequent message will be sent to the "next"
endpoint in the PUSH's collection.  Only one PULL socket gets any
particular message.

PUB sends a message to all SUB endpoints which has subscribed to the
"topic" of the message.  The topic is simply a prefix match against
the initial bytes of the message.  To receive messages a SUB must
subscribe to topics individually or to all (the empty topic).

** High Water Mark Behavior

In ZeroMQ like any system that transmits data asynchronously there are
data buffers that smooth out the spikes in transmission rate.  ZeroMQ
has both send and receive buffers managed by the sockets.  These
buffers can become full if they reach their high water mark (HWM).

When the HWM is reached one of only two things must happen:

- block :: transmission must halt and the resume when possible
- drop :: transmission must skip data until it can be accepted again

This certainly is not specific to ZeroMQ.  When one hits a full queue,
something's gotta give.  Either you abandon entering the queue or you
must wait where you are until there is again room in the queue.

Of course, one can make the queue larger or employ faster network or
computers.  This will minimize the likelihood of hitting the HWM but
it does not remove the need to design for the eventuality of hitting
it.  Three is no magic and we can not rely on hope.

Each ZeroMQ socket pattern chooses between *block* and *drop* policy.
This policy is thus linked to the multiplicity routing policy
described above.

PUB/SUB will *drop* messages if the HWM is reached.  This is very useful
if the transmission should avoid forcing back pressure onto the PUB
side.  This (along with drop detection) makes most sense for trigger
transmission in protoDUNE and DUNE.  If HWM is reached, we do not want
to "back up" the data flow all the way to the hardware.  We have to
deal with it at the source of the problem.

PAIR, again 1-to-1, blocks.  The HWM is actually the sum of the HWM of
both PAIRs.  Conceptually, it's a pipeline.  If the pipe is full, no
new messages will be added until room is made.

PUSH/PULL blocks.  Each PULL has its own HWM.  A send to PUSH will
continue until all PULL sockets have reached HWM and then further
until the PUSH HWM is reached.




* Tuning and Exception Handling

The document [[./docs/tuning.org][tuning]] collects some information that will help
understand how the PTMP network behaves, where things can be tuned to
meet different goals and also some known features of a high
performance, asynchronous communication network that are best taken
into account.

